{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from keras_nlp.layers import TransformerEncoder, TransformerDecoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class splitData():\n",
    "    def __init__(self):\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "    \n",
    "    def split(self, x, y):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['district_encoder',\\\n",
    "        'area',\\\n",
    "        'new_num_floors',\\\n",
    "        'new_bedrooms',\\\n",
    "        'houseTypes_Bán Luxury home',\\\n",
    "        'houseTypes_Bán Nhà',\\\n",
    "        'houseTypes_Bán Nhà cổ',\\\n",
    "        'houseTypes_Bán Nhà mặt phố',\\\n",
    "        'houseTypes_Bán Nhà riêng']\n",
    "\n",
    "df = pd.read_excel('HCM_data.xlsx')\n",
    "df_tranform = pd.DataFrame(data = Normalizer().fit_transform(\\\n",
    "    StandardScaler().fit_transform(df.loc[:, features].values)), columns = features)\n",
    "y = df['price'].values\n",
    "x = df_tranform[features].values\n",
    "data = splitData()\n",
    "data.split(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelRNN(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = keras.layers.Dense(128, activation='relu') #fully connected networks\n",
    "        self.dense2 = keras.layers.Dense(128, activation='relu')\n",
    "        self.dense3 = keras.layers.Dense(128, activation='relu')\n",
    "        self.dense4 = keras.layers.Dense(1, activation= 'relu')\n",
    "        \n",
    "        self.model = Sequential([self.dense1, self.dense2, self.dense3, self.dense4])\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def compile(self, optimizer=\"adam\", loss=\"mse\"):\n",
    "        super().compile(optimizer, loss)\n",
    "        \n",
    "    def fit(self, data, epochs = 500, batch_size=128):\n",
    "        super().fit(data.X_train, data.y_train, epochs, batch_size)\n",
    "        \n",
    "    def evaluation(self, data):\n",
    "        y_pred = self.predict(data.X_test)\n",
    "        \n",
    "        mse = mean_squared_error(data.y_test, y_pred)\n",
    "        mae = mean_absolute_error(data.y_test, y_pred)\n",
    "        evs = explained_variance_score(data.y_test, y_pred)\n",
    "        \n",
    "        print(\"MSE: \", mse)\n",
    "        print(\"MAE: \", mae)\n",
    "        print(\"variance: \", evs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 541.4588\n",
      "Epoch 2/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 541.2914\n",
      "Epoch 3/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 540.8826\n",
      "Epoch 4/128\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 540.0930\n",
      "Epoch 5/128\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 539.1042\n",
      "Epoch 6/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 538.0129\n",
      "Epoch 7/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 536.8526\n",
      "Epoch 8/128\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 535.6166\n",
      "Epoch 9/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 534.2833\n",
      "Epoch 10/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 532.8297\n",
      "Epoch 11/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 531.2378\n",
      "Epoch 12/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 529.4854\n",
      "Epoch 13/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 527.5544\n",
      "Epoch 14/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 525.4259\n",
      "Epoch 15/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 523.0793\n",
      "Epoch 16/128\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 520.5016\n",
      "Epoch 17/128\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 517.6844\n",
      "Epoch 18/128\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 514.6160\n",
      "Epoch 19/128\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 511.2905\n",
      "Epoch 20/128\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 507.7057\n",
      "Epoch 21/128\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 503.8616\n",
      "Epoch 22/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 499.7639\n",
      "Epoch 23/128\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 495.4297\n",
      "Epoch 24/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 490.8855\n",
      "Epoch 25/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 486.1764\n",
      "Epoch 26/128\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 481.3683\n",
      "Epoch 27/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 476.5412\n",
      "Epoch 28/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 471.7886\n",
      "Epoch 29/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 467.2283\n",
      "Epoch 30/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 462.9914\n",
      "Epoch 31/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 459.2204\n",
      "Epoch 32/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 456.0464\n",
      "Epoch 33/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 453.5603\n",
      "Epoch 34/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 451.7795\n",
      "Epoch 35/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 450.6329\n",
      "Epoch 36/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 449.9579\n",
      "Epoch 37/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 449.5363\n",
      "Epoch 38/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 449.1383\n",
      "Epoch 39/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 448.5767\n",
      "Epoch 40/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 447.7390\n",
      "Epoch 41/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 446.6005\n",
      "Epoch 42/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 445.2104\n",
      "Epoch 43/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 443.6637\n",
      "Epoch 44/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 442.0717\n",
      "Epoch 45/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 440.5361\n",
      "Epoch 46/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 439.1338\n",
      "Epoch 47/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 437.9082\n",
      "Epoch 48/128\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 436.8710\n",
      "Epoch 49/128\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 436.0053\n",
      "Epoch 50/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 435.2780\n",
      "Epoch 51/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 434.6480\n",
      "Epoch 52/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 434.0733\n",
      "Epoch 53/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 433.5179\n",
      "Epoch 54/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 432.9549\n",
      "Epoch 55/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 432.3659\n",
      "Epoch 56/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 431.7441\n",
      "Epoch 57/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 431.0912\n",
      "Epoch 58/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 430.4155\n",
      "Epoch 59/128\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 429.7301\n",
      "Epoch 60/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 429.0520\n",
      "Epoch 61/128\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 428.3955\n",
      "Epoch 62/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 427.7723\n",
      "Epoch 63/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 427.1892\n",
      "Epoch 64/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 426.6469\n",
      "Epoch 65/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 426.1398\n",
      "Epoch 66/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 425.6599\n",
      "Epoch 67/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 425.1966\n",
      "Epoch 68/128\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 424.7393\n",
      "Epoch 69/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 424.2803\n",
      "Epoch 70/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 423.8145\n",
      "Epoch 71/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 423.3422\n",
      "Epoch 72/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 422.8672\n",
      "Epoch 73/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 422.3943\n",
      "Epoch 74/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 421.9298\n",
      "Epoch 75/128\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 421.4798\n",
      "Epoch 76/128\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 421.0470\n",
      "Epoch 77/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 420.6311\n",
      "Epoch 78/128\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 420.2305\n",
      "Epoch 79/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 419.8428\n",
      "Epoch 80/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 419.4620\n",
      "Epoch 81/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 419.0858\n",
      "Epoch 82/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 418.7116\n",
      "Epoch 83/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 418.3383\n",
      "Epoch 84/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 417.9664\n",
      "Epoch 85/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 417.5965\n",
      "Epoch 86/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 417.2318\n",
      "Epoch 87/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 416.8736\n",
      "Epoch 88/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 416.5225\n",
      "Epoch 89/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 416.1775\n",
      "Epoch 90/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 415.8365\n",
      "Epoch 91/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 415.4975\n",
      "Epoch 92/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 415.1595\n",
      "Epoch 93/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 414.8217\n",
      "Epoch 94/128\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 414.4829\n",
      "Epoch 95/128\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 414.1425\n",
      "Epoch 96/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 413.7999\n",
      "Epoch 97/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 413.4567\n",
      "Epoch 98/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 413.1131\n",
      "Epoch 99/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 412.7693\n",
      "Epoch 100/128\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 412.4254\n",
      "Epoch 101/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 412.0812\n",
      "Epoch 102/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 411.7361\n",
      "Epoch 103/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 411.3875\n",
      "Epoch 104/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 411.0367\n",
      "Epoch 105/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 410.6838\n",
      "Epoch 106/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 410.3289\n",
      "Epoch 107/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 409.9712\n",
      "Epoch 108/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 409.6111\n",
      "Epoch 109/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 409.2492\n",
      "Epoch 110/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 408.8838\n",
      "Epoch 111/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 408.5131\n",
      "Epoch 112/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 408.1386\n",
      "Epoch 113/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 407.7616\n",
      "Epoch 114/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 407.3821\n",
      "Epoch 115/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 406.9999\n",
      "Epoch 116/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 406.6149\n",
      "Epoch 117/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 406.2258\n",
      "Epoch 118/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 405.8333\n",
      "Epoch 119/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 405.4382\n",
      "Epoch 120/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 405.0412\n",
      "Epoch 121/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 404.6412\n",
      "Epoch 122/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 404.2375\n",
      "Epoch 123/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 403.8308\n",
      "Epoch 124/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 403.4218\n",
      "Epoch 125/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 403.0085\n",
      "Epoch 126/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 402.5903\n",
      "Epoch 127/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 402.1680\n",
      "Epoch 128/128\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 401.7411\n",
      "15/15 [==============================] - 0s 622us/step\n",
      "MSE:  513.8100990519181\n",
      "MAE:  5.7835864978092975\n",
      "variance:  0.04837617271625971\n"
     ]
    }
   ],
   "source": [
    "model_rnn = ModelRNN()\n",
    "model_rnn.compile(optimizer='adam', loss='mse')\n",
    "model_rnn.fit(data, epochs=10000000, batch_size =128)\n",
    "model_rnn.evaluation(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Sequential([\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "        ])\n",
    "\n",
    "        self.decoder = Sequential([\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu')\n",
    "        ])\n",
    "        self.dense = Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_outputs = self.decoder(encoder_outputs)\n",
    "        outputs = self.dense(decoder_outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def compile(self, optimizer='adam', loss='mse'):\n",
    "        super().compile(optimizer, loss)\n",
    "        \n",
    "    def fit(self, x_train, y_train, epochs=1000, batch_size =128):\n",
    "        super().fit(x_train, y_train, epochs, batch_size)\n",
    "    \n",
    "    def evaluation(self, data):\n",
    "        y_pred = self.predict(data.X_test)\n",
    "        \n",
    "        mse = mean_squared_error(data.y_test, y_pred)\n",
    "        mae = mean_absolute_error(data.y_test, y_pred)\n",
    "        evs = explained_variance_score(data.y_test, y_pred)\n",
    "        \n",
    "        print(\"MSE: \", mse)\n",
    "        print(\"MAE: \", mae)\n",
    "        print(\"variance: \", evs)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "9/9 [==============================] - 1s 3ms/step - loss: 527.4604\n",
      "Epoch 2/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 444.6162\n",
      "Epoch 3/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 432.5425\n",
      "Epoch 4/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 418.3723\n",
      "Epoch 5/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 407.1867\n",
      "Epoch 6/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 398.5597\n",
      "Epoch 7/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 401.4498\n",
      "Epoch 8/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 395.7618\n",
      "Epoch 9/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 392.2677\n",
      "Epoch 10/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 390.8188\n",
      "Epoch 11/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 386.2924\n",
      "Epoch 12/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 384.7236\n",
      "Epoch 13/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 373.2153\n",
      "Epoch 14/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 368.9081\n",
      "Epoch 15/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 369.0862\n",
      "Epoch 16/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 372.2315\n",
      "Epoch 17/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 368.5113\n",
      "Epoch 18/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 363.0372\n",
      "Epoch 19/128\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 352.6432\n",
      "Epoch 20/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 350.3208\n",
      "Epoch 21/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 345.8730\n",
      "Epoch 22/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 339.9641\n",
      "Epoch 23/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 335.8299\n",
      "Epoch 24/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 334.9860\n",
      "Epoch 25/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 333.0798\n",
      "Epoch 26/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 328.7667\n",
      "Epoch 27/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 325.3691\n",
      "Epoch 28/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 325.6322\n",
      "Epoch 29/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 323.5797\n",
      "Epoch 30/128\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 321.1537\n",
      "Epoch 31/128\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 320.9258\n",
      "Epoch 32/128\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 321.1921\n",
      "Epoch 33/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 319.9983\n",
      "Epoch 34/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 320.4757\n",
      "Epoch 35/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 323.8981\n",
      "Epoch 36/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 320.0197\n",
      "Epoch 37/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 319.6241\n",
      "Epoch 38/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 317.8807\n",
      "Epoch 39/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 315.1343\n",
      "Epoch 40/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 315.9559\n",
      "Epoch 41/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 318.8279\n",
      "Epoch 42/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 314.0385\n",
      "Epoch 43/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 312.8418\n",
      "Epoch 44/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 312.5155\n",
      "Epoch 45/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 312.3309\n",
      "Epoch 46/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 311.9624\n",
      "Epoch 47/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 310.1021\n",
      "Epoch 48/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.5680\n",
      "Epoch 49/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 310.2558\n",
      "Epoch 50/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.8011\n",
      "Epoch 51/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.0007\n",
      "Epoch 52/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.1571\n",
      "Epoch 53/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.7031\n",
      "Epoch 54/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.7899\n",
      "Epoch 55/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 306.0107\n",
      "Epoch 56/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 314.9667\n",
      "Epoch 57/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 313.7701\n",
      "Epoch 58/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 308.1755\n",
      "Epoch 59/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 313.3294\n",
      "Epoch 60/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 313.5281\n",
      "Epoch 61/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 314.4624\n",
      "Epoch 62/128\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 317.0652\n",
      "Epoch 63/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 332.6370\n",
      "Epoch 64/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 318.1651\n",
      "Epoch 65/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.7422\n",
      "Epoch 66/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.0821\n",
      "Epoch 67/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 306.2349\n",
      "Epoch 68/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 304.4619\n",
      "Epoch 69/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 300.9855\n",
      "Epoch 70/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 300.2817\n",
      "Epoch 71/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 300.8258\n",
      "Epoch 72/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 302.3989\n",
      "Epoch 73/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 297.6516\n",
      "Epoch 74/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 299.7896\n",
      "Epoch 75/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 313.9149\n",
      "Epoch 76/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 304.6083\n",
      "Epoch 77/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 312.9045\n",
      "Epoch 78/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 306.1083\n",
      "Epoch 79/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 300.5267\n",
      "Epoch 80/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 299.3939\n",
      "Epoch 81/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 297.2297\n",
      "Epoch 82/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 298.0257\n",
      "Epoch 83/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 296.9637\n",
      "Epoch 84/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 292.7228\n",
      "Epoch 85/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 292.6836\n",
      "Epoch 86/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 293.4501\n",
      "Epoch 87/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 300.0387\n",
      "Epoch 88/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 294.3076\n",
      "Epoch 89/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 291.0536\n",
      "Epoch 90/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 292.7140\n",
      "Epoch 91/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 292.3346\n",
      "Epoch 92/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 295.3446\n",
      "Epoch 93/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 294.0094\n",
      "Epoch 94/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 289.7341\n",
      "Epoch 95/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 285.8934\n",
      "Epoch 96/128\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 285.3169\n",
      "Epoch 97/128\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 289.3331\n",
      "Epoch 98/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 283.7271\n",
      "Epoch 99/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 286.3019\n",
      "Epoch 100/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 284.7628\n",
      "Epoch 101/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 281.3885\n",
      "Epoch 102/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 280.7183\n",
      "Epoch 103/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 281.4101\n",
      "Epoch 104/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 302.0901\n",
      "Epoch 105/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 293.4885\n",
      "Epoch 106/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 293.7283\n",
      "Epoch 107/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 291.4433\n",
      "Epoch 108/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 282.4521\n",
      "Epoch 109/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 279.9576\n",
      "Epoch 110/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 275.6647\n",
      "Epoch 111/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 274.1616\n",
      "Epoch 112/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 274.5897\n",
      "Epoch 113/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 271.5312\n",
      "Epoch 114/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 273.3036\n",
      "Epoch 115/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 274.2008\n",
      "Epoch 116/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 280.4748\n",
      "Epoch 117/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 272.9435\n",
      "Epoch 118/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 265.7145\n",
      "Epoch 119/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 265.4552\n",
      "Epoch 120/128\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 269.5295\n",
      "Epoch 121/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 268.9581\n",
      "Epoch 122/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 275.6420\n",
      "Epoch 123/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 286.6777\n",
      "Epoch 124/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 295.4520\n",
      "Epoch 125/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 280.5418\n",
      "Epoch 126/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 259.0318\n",
      "Epoch 127/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 262.9705\n",
      "Epoch 128/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 292.7551\n",
      "15/15 [==============================] - 0s 725us/step\n",
      "MSE:  446.1833048511941\n",
      "MAE:  5.219858334408012\n",
      "variance:  0.17541580027347092\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(data.X_train, data.y_train, epochs=500, batch_size =128)\n",
    "model.evaluation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(keras.Model):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, dropout_rate)\n",
    "        self.decoder = TransformerDecoder(num_layers, d_model, num_heads, dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        encoder_outputs = self.encoder(inputs, training=training)\n",
    "        decoder_outputs = self.decoder(encoder_outputs, training=training)\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "class TransformerEncoder_(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dropout_rate):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.attention_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.attention_layers.append(\n",
    "                keras.layers.MultiHeadAttention(\\\n",
    "                    num_heads = num_heads,\\\n",
    "                    key_dim = d_model,\\\n",
    "                    dropout = dropout_rate\\\n",
    "                )\\\n",
    "            )\n",
    "        self.feed_forward_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.feed_forward_layers.append(\n",
    "                keras.layers.Dense(\\\n",
    "                    d_model,\\\n",
    "                    activation = \"relu\"\\\n",
    "                )\\\n",
    "            )\n",
    "            \n",
    "    def call(self, inputs, training=False):\n",
    "        for attention_layer in self.attention_layers:\n",
    "            inputs = attention_layer(inputs, inputs, training=training)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "            \n",
    "        for feed_forward_layer in self.feed_forward_layers:\n",
    "            inputs = feed_forward_layer(inputs)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "            \n",
    "        return inputs\n",
    "\n",
    "class TransformerDecoder_(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dropout_rate):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.attention_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.attention_layers.append(\n",
    "                keras.layers.MultiHeadAttention(\n",
    "                    num_heads = num_heads,\n",
    "                    key_dim = d_model,\n",
    "                    dropout = dropout_rate\n",
    "                )\n",
    "            )\n",
    "        self.feed_forward_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.feed_forward_layers.append(\\\n",
    "                keras.layers.Dense(\\\n",
    "                    d_model,\\\n",
    "                    activation=\"relu\"\\\n",
    "                )\\\n",
    "            )\n",
    "            \n",
    "        self.final_layer = keras.layers.Dense(\n",
    "            1, # kích thước output\n",
    "            activation=\"softmax\" \n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, encoder_outputs, training=False):\n",
    "        for attention_layer in self.attention_layers:\n",
    "            inputs = attention_layer(inputs, encoder_outputs, training=training)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        for feed_forward_layer in self.feed_forward_layers:\n",
    "            inputs = feed_forward_layer(inputs)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        outputs = self.final_layer(inputs)\n",
    "        return outputs\n",
    "\n",
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_input = Input(shape=(None, 9))\n",
    "        \n",
    "        self.encoder = TransformerEncoder(intermediate_dim=64, num_heads=8)\n",
    "        \n",
    "        self.encoder_output = encoder(encoder_input)\n",
    "        \n",
    "        self.decoder_input = Input(shape=(None, 9))\n",
    "        self.decoder = TransformerDecoder(\\\n",
    "            intermediate_dim=64, num_heads=8\n",
    "        )\n",
    "        self.decoder_output = decoder(decoder_input, encoder_output)\n",
    "        self.model = Model(self.encoder_input, self.encoder, self.decoder_output, self.decoder)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs=1000, batch_size =128):\n",
    "        super().fit(x_train, y_train, epochs, batch_size)\n",
    "    2\n",
    "    def evaluation(self, data):\n",
    "        y_pred = self.predict(data.X_test)\n",
    "        \n",
    "        mse = mean_squared_error(data.y_test, y_pred)\n",
    "        mae = mean_absolute_error(data.y_test, y_pred)\n",
    "        evs = explained_variance_score(data.y_test, y_pred)\n",
    "        \n",
    "        print(\"MSE: \", mse)\n",
    "        print(\"MAE: \", mae)\n",
    "        print(\"variance: \", evs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_tran \u001b[39m=\u001b[39m TransformerModel()\n\u001b[1;32m      2\u001b[0m model_tran\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model_tran\u001b[39m.\u001b[39mfit(data, epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, batch_size \u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "Cell \u001b[0;32mIn[90], line 94\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_input \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m9\u001b[39m))\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m TransformerEncoder(intermediate_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_output \u001b[39m=\u001b[39m encoder(encoder_input)\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_input \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m9\u001b[39m))\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m TransformerDecoder(\\\n\u001b[1;32m     98\u001b[0m     intermediate_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "model_tran = TransformerModel()\n",
    "model_tran.compile(optimizer='adam', loss='mse')\n",
    "model_tran.fit(data, epochs=500, batch_size =128)\n",
    "model_tran.evaluation(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
